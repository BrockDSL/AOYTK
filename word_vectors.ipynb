{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5575e783-e884-4872-9ecf-3426ea185114",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065deb8c-8488-4ad8-b299-acfad076c958",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word embeddings/vectors \n",
    "This notebook creates word vectors for a provided text. Word vectors are numerical representations of words in a text and are used for determining semantic relationships between words. In this notebook, two different methods can be used and a number of different parameters can be specified for generating the vectors. More importantly instead of trying to guess what parameters are optimal, this notebook will create vectors based on all the permutations of the possible parameters, thus giving you the opportunity to see what works best for your own dataset.  The word vectors can be exported for visualization or used to augment other NLP tasks such as document similarity or classification.\n",
    "\n",
    "Read more about word embeddings here [https://www.coveo.com/blog/word2vec-explained/](https://www.coveo.com/blog/word2vec-explained/)\n",
    "\n",
    "This notebook uses both Word2Vec and FastText, for which you can find more about [https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/) and [https://fasttext.cc/](https://fasttext.cc/)\n",
    "\n",
    "Visualization of the word vectors can be done using [http://projector.tensorflow.org/](http://projector.tensorflow.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8159f3f-0b15-43aa-ad46-bf5be14de462",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A few notes before starting\n",
    "Before you start generating word vectors you need to get your data into the right shape. On the most basic level this means turning your collection of text into a single text file and making sure that each sentence begins on its own line. \n",
    "\n",
    "In addition doing the any of the following will increase the accuracy of the translation process from text to vectors\n",
    " - \"normalizing\" text such as turning semantically equivalent but lexically different characters to a single representation (unicode to ascii)\n",
    " - removing unwanted symbols\n",
    " - lowercasing all text\n",
    " - removing stopwords\n",
    " - removing numbers (depending if they are significant to your dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ce411-4306-43ed-b8a4-578379b432b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the following three cells to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cb79d9-c4b1-4b7f-a3f4-2e6e999946a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets \n",
    "import multiprocessing\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, FastText \n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "pd.set_option('max_colwidth', 1600)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "MODEL_PATH = './models'\n",
    "models = None\n",
    "sentences = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a66f6ec-8b04-4883-a80f-06617f0b5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def permutations(parameters):\n",
    "    '''return all permutations of model parameters'''\n",
    "    \n",
    "    keys = parameters.keys()\n",
    "    values = parameters.values()\n",
    "    return [dict(zip(keys,tup)) for tup in product(*values)]\n",
    "\n",
    "def gen_file_name(model_name, kwargs):\n",
    "    '''generate the a model's filename'''\n",
    "    \n",
    "    filename = model_name + ''.join([f\"_{k}_{v}\" for k,v in kwargs.items()]) + '.bin'\n",
    "    filename = filename.replace('fasttext','ft')\\\n",
    "                       .replace('word2vec','w2v')\\\n",
    "                       .replace('vector_size','vs')\\\n",
    "                       .replace('window','wn')\n",
    "    filename = re.sub('_workers_\\d\\d?','',filename)\n",
    "    return filename\n",
    "\n",
    "def gen_w2v_models(save_dir, models, args, parameters, constant_kwargs):\n",
    "    '''generate word vector models'''\n",
    "    \n",
    "    total_iterations = len(models) * len(parameters)\n",
    "    models = {}\n",
    "\n",
    "    with tqdm(total=total_iterations) as pbar:\n",
    "        for name, model in models: \n",
    "            for kwargs in parameters:\n",
    "                kwargs.update(constant_kwargs)\n",
    "                filename = gen_file_name(name, kwargs)\n",
    "                pbar.set_description(f\"Generating {filename}\")\n",
    "                m = model(*args,**kwargs)\n",
    "                m.wv.save_word2vec_format(os.path.join(save_dir, filename), binary=True)\n",
    "                models[filename] = m.wv\n",
    "                pbar.update(1)\n",
    "\n",
    "    return models\n",
    "\n",
    "#Load models and put results into a dataframe\n",
    "def load_w2v_models(path):\n",
    "    return {os.path.basename(model).split('.bin')[0]: KeyedVectors.load_word2vec_format(model, binary=True) \n",
    "            for model in glob(os.path.join(path,'*.bin'))}\n",
    "\n",
    "def return_similar(word, topn=5):\n",
    "    '''create a dataframe that shows the topn similar words to 'word' across all models'''\n",
    "    df = pd.DataFrame()\n",
    "    sorted_models = sorted(models.items(), key=lambda tup: tup[0])\n",
    "    for name, model in sorted_models:\n",
    "        name = name.replace('fasttext','ft')\\\n",
    "                   .replace('word2vec','w2v')\\\n",
    "                   .replace('vector_size','vs')\\\n",
    "                   .replace('window','wn')\\\n",
    "                   .replace('_workers_6','')\n",
    "        df[name] = [f\"{word} {percent:.3f}\" for word,percent in model.most_similar(topn=topn,positive=[word])]\n",
    "    return df.T\n",
    "\n",
    "def export_wv_tensor_ep(tensor_ep_dir, models):\n",
    "    '''export word vectors from a model for visualization in http://projector.tensorflow.org/'''\n",
    "\n",
    "    if not os.path.exists(tensor_ep_dir):\n",
    "        os.mkdir(tensor_ep_dir)\n",
    "    \n",
    "    print('Exporting word vectors')\n",
    "    for model_name in models:\n",
    "        print(model_name)\n",
    "        with open(os.path.join(tensor_ep_dir, model_name) + '_words.tsv', 'w') as metadata_f:\n",
    "            vector_names = sorted(models[model_name].key_to_index.keys())\n",
    "            metadata_f.write('\\n'.join(vector_names))\n",
    "\n",
    "        with open(os.path.join(tensor_ep_dir, model_name) + '_vecs.tsv', 'w') as vectors_f:\n",
    "            vectors = ['\\t'.join(map(str, models[model_name][vn])) + '\\n' for vn in vector_names] \n",
    "            vectors_f.writelines(vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742e10d-3092-47bf-8d45-3a559dd7150e",
   "metadata": {},
   "source": [
    "Once the cell below is run it creates several widgets that allow you to set parameters and select the vector creation algorithms. \n",
    "\n",
    "Either \n",
    "1. \"Load your data\" and then \"Generate models\". \n",
    "2. Or \"Load existing models\" that you have generated previously.\n",
    "\n",
    "A word of caution, generating models can take quite a bit of time (30 minutes+)\n",
    "\n",
    "Also some notes on what the various parameters mean.\n",
    "\n",
    "- [Window size](https://stackoverflow.com/questions/22272370/word2vec-effect-of-window-size-used/30447723#30447723): Larger windows tend to capture more topic/domain information: what other words (of any type) are used in related discussions? Smaller windows tend to capture more about word itself: what other words are functionally similar?\n",
    "- Vector size: The number of dimensions for each vector. The more dimensions there are, the more information there is for situating relationships between the vectors (within reason). 200 or 300 are common numbers.\n",
    "- Word2Vec/FastText: Two different methods of turning words into word vectors. FastText is a newer method.\n",
    "\n",
    "Enter in values separated by commas to try more than one parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef04d08-acf0-4c78-a8a3-24446a036b23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff67a3616ec45c195b3c5164f3fb189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='doc_sent_file.txt', description='Data filename:', placeholder='Enterâ€¦"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "data_textbox = widgets.Text(\n",
    "    value='doc_sent_file.txt',\n",
    "    placeholder='Enter data filename here',\n",
    "    description='Data filename:',\n",
    "    disabled=False   \n",
    ")\n",
    "data_load_btn = button = widgets.Button(\n",
    "    description='Load data',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "@output.capture(clear_output=True)\n",
    "def dlb_callback(_):\n",
    "    global sentences\n",
    "    #Load data here and change color of button to red if failure/green if success\n",
    "    data_path = data_textbox.value\n",
    "   \n",
    "    try:\n",
    "        with open(data_path) as dsf:\n",
    "            max_sent_length = max([len(line) for line in dsf.readlines()])\n",
    "\n",
    "        sentences = LineSentence(data_path, max_sentence_length=max_sent_length)\n",
    "        data_load_btn.style.button_color = 'lightgreen'\n",
    "        toggle_gm_widgets(False)\n",
    "        print('data loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        data_load_btn.style.button_color = 'red'\n",
    "        print('path not found')\n",
    "    \n",
    "data_load_btn.on_click(dlb_callback)\n",
    "    \n",
    "model_dir = widgets.Text(\n",
    "    value=MODEL_PATH,\n",
    "    placeholder=MODEL_PATH,\n",
    "    description='Model directory:',\n",
    "    disabled=False   \n",
    ")\n",
    "model_load_btn = widgets.Button(\n",
    "    description='Load existing models',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "@output.capture(clear_output=True)\n",
    "def mlb_callback(_): \n",
    "    global models \n",
    "    print(\"Loading models\")\n",
    "    models = load_w2v_models(model_dir.value)\n",
    "    if not models:\n",
    "        model_load_btn.style.button_color = 'red'\n",
    "        print('bad model directory')\n",
    "    else:\n",
    "        enable_model_functions()\n",
    "        model_load_btn.style.button_color = 'lightgreen'\n",
    "        for model in models:\n",
    "            print(model)\n",
    "        \n",
    "model_load_btn.on_click(mlb_callback)\n",
    "\n",
    "mgp_label = widgets.Label(value=\"Model generation parameters\")\n",
    "vs_param = widgets.Text(\n",
    "    value='50,100,200',\n",
    "    description='Vector size:',\n",
    "    disabled=False   \n",
    ")\n",
    "ws_param = widgets.Text(\n",
    "    value='2,5,8',\n",
    "    description='Window size:',\n",
    "    disabled=False   \n",
    ")\n",
    "cbow_param = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='CBOW',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "sg_param = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Skip Gram',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "ag_params = widgets.SelectMultiple(\n",
    "    options=['Word2Vec','FastText'],\n",
    "    value=['Word2Vec','FastText'],\n",
    "    description='Algorithms',\n",
    "    disabled=False\n",
    ")\n",
    "save_model_dir = widgets.Text(\n",
    "    value=MODEL_PATH,\n",
    "    placeholder=MODEL_PATH,\n",
    "    description='Model directory:',\n",
    "    disabled=False   \n",
    ")\n",
    "gm_btn = widgets.Button(\n",
    "    description='Generate models',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "@output.capture(clear_output=True)\n",
    "def gm_callback(_):\n",
    "    global models\n",
    "    \n",
    "    try:\n",
    "        vs = [int(num) for num in vs_param.value.split(',')]\n",
    "        ws = [int(num) for num in ws_param.value.split(',')]\n",
    "        \n",
    "        sg = []\n",
    "        if cbow_param.value: sg.append(0)\n",
    "        if sg_param.value: sg.append(1)\n",
    "        if not sg: \n",
    "            raise ValueError(\"One of CBOW or Skip Gram must be selected\")\n",
    "            \n",
    "        ag = []\n",
    "        if 'Word2Vec' in ag_params.value:\n",
    "            ag.append(('word2vec',Word2Vec))\n",
    "        if 'FastText' in ag_params.value:\n",
    "            ag.append(('fasttext',FastText))\n",
    "\n",
    "        if not os.path.isdir(save_model_dir.value):\n",
    "            raise ValueError(\"Could not save models to non directory path \" + save_model_dir.value)\n",
    "            \n",
    "        parameters = permutations({'vector_size':vs, 'window':ws, 'sg':sg})\n",
    "\n",
    "        models = gen_w2v_models(save_model_dir,\n",
    "                               ag,\n",
    "                               [sentences],\n",
    "                               parameters,\n",
    "                               {\"workers\":multiprocessing.cpu_count()})\n",
    "        \n",
    "        enable_model_functions()\n",
    "        gm_btn.style.button_color = 'lightgreen'\n",
    "    except Exception as e:\n",
    "        gm_btn.style.button_color = 'red'\n",
    "        raise(e)\n",
    "\n",
    "gm_btn.on_click(gm_callback)\n",
    "\n",
    "gen_models_widgets = [mgp_label, vs_param, ws_param, cbow_param, sg_param, ag_params, gm_btn] \n",
    "gmw_area=widgets.VBox(gen_models_widgets)\n",
    "\n",
    "def toggle_gm_widgets(off=True):\n",
    "    for widget in gen_models_widgets:\n",
    "        widget.disabled=off\n",
    "\n",
    "toggle_gm_widgets(True)\n",
    "\n",
    "compare_topn = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter word to compare here',\n",
    "    description='Compare to...',\n",
    "    disabled=True   \n",
    ")\n",
    "\n",
    "export_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "export_wv = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter directory to export word vectors',\n",
    "    description='Export path',\n",
    "    disabled=True   \n",
    ")\n",
    "export_wv_btn = widgets.Button(\n",
    "    description='Export vectors',\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "@export_output.capture(clear_output=True)\n",
    "def export_callback(_):\n",
    "    try:\n",
    "        export_wv_tensor_ep(export_wv.value, models)\n",
    "        export_wv_btn.style.button_color = 'lightgreen'\n",
    "    except Exception as e:\n",
    "        export_wv_btn.style.button_color = 'red'\n",
    "        \n",
    "export_wv_btn.on_click(export_callback)\n",
    "\n",
    "def enable_model_functions():\n",
    "    compare_topn.disabled=False\n",
    "    export_wv.disabled=False\n",
    "    export_wv_btn.disabled=False\n",
    "    \n",
    "widgets.VBox([widgets.HBox([data_textbox, data_load_btn]), \n",
    "              widgets.HBox([model_dir, model_load_btn]),\n",
    "              gmw_area,\n",
    "              output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e28d6a-0cd3-4678-a73c-5c7335f6ac2c",
   "metadata": {},
   "source": [
    "This code cell exports vectors for visualization in [http://projector.tensorflow.org/](http://projector.tensorflow.org/). It is enabled once existing models have been loaded or generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9ff798f-0d2c-450d-bd15-f035028b5884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dac78ef2f514c9dbf35f136ae804f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='test', description='Export path', placeholder='Enter directory to exâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.VBox([widgets.HBox([export_wv, export_wv_btn]), export_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d119a-acb9-4767-8177-97f1988dbcc3",
   "metadata": {},
   "source": [
    "Run the cell below and enter in a word, then run the cell again to see how it compares with other words across the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6713d2de-b340-423b-9aff-7264c81f4bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee04ad2317b14855b037620cfdfb389e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='well-educated', description='Compare to...', placeholder='Enter word to compare here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ft_vs_100_wn_2_sg_0</th>\n",
       "      <td>well-ordered 0.914</td>\n",
       "      <td>educated 0.903</td>\n",
       "      <td>well-planned 0.897</td>\n",
       "      <td>well-trained 0.896</td>\n",
       "      <td>well-informed 0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_100_wn_2_sg_1</th>\n",
       "      <td>educated 0.855</td>\n",
       "      <td>uneducated 0.841</td>\n",
       "      <td>well-ordered 0.810</td>\n",
       "      <td>well-trained 0.802</td>\n",
       "      <td>well-armed 0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_100_wn_5_sg_0</th>\n",
       "      <td>educated 0.905</td>\n",
       "      <td>uneducated 0.878</td>\n",
       "      <td>well-trained 0.872</td>\n",
       "      <td>well-planned 0.868</td>\n",
       "      <td>well-regulated 0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_100_wn_5_sg_1</th>\n",
       "      <td>educated 0.874</td>\n",
       "      <td>uneducated 0.854</td>\n",
       "      <td>well-ordered 0.792</td>\n",
       "      <td>well-endowed 0.759</td>\n",
       "      <td>well-informed 0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_100_wn_8_sg_0</th>\n",
       "      <td>educated 0.905</td>\n",
       "      <td>uneducated 0.875</td>\n",
       "      <td>well-trained 0.860</td>\n",
       "      <td>well-planned 0.852</td>\n",
       "      <td>well-advised 0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_100_wn_8_sg_1</th>\n",
       "      <td>educated 0.883</td>\n",
       "      <td>uneducated 0.854</td>\n",
       "      <td>well-to-do 0.779</td>\n",
       "      <td>well-ordered 0.774</td>\n",
       "      <td>educate 0.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_200_wn_2_sg_0</th>\n",
       "      <td>educated 0.893</td>\n",
       "      <td>well-ordered 0.882</td>\n",
       "      <td>uneducated 0.879</td>\n",
       "      <td>well-advised 0.875</td>\n",
       "      <td>well-trained 0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_200_wn_2_sg_1</th>\n",
       "      <td>educated 0.856</td>\n",
       "      <td>uneducated 0.832</td>\n",
       "      <td>well-trained 0.756</td>\n",
       "      <td>well-ordered 0.754</td>\n",
       "      <td>well-informed 0.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_200_wn_5_sg_0</th>\n",
       "      <td>educated 0.895</td>\n",
       "      <td>uneducated 0.868</td>\n",
       "      <td>well-planned 0.848</td>\n",
       "      <td>well-trained 0.847</td>\n",
       "      <td>well-regulated 0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_200_wn_5_sg_1</th>\n",
       "      <td>educated 0.854</td>\n",
       "      <td>uneducated 0.828</td>\n",
       "      <td>well-trained 0.748</td>\n",
       "      <td>well-ordered 0.729</td>\n",
       "      <td>educate 0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_200_wn_8_sg_0</th>\n",
       "      <td>educated 0.894</td>\n",
       "      <td>uneducated 0.858</td>\n",
       "      <td>well-trained 0.842</td>\n",
       "      <td>well-planned 0.825</td>\n",
       "      <td>well-orchestrated 0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_200_wn_8_sg_1</th>\n",
       "      <td>educated 0.871</td>\n",
       "      <td>uneducated 0.846</td>\n",
       "      <td>educate 0.746</td>\n",
       "      <td>well-ordered 0.739</td>\n",
       "      <td>well-trained 0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_50_wn_2_sg_0</th>\n",
       "      <td>well-planned 0.922</td>\n",
       "      <td>well-ordered 0.921</td>\n",
       "      <td>educated 0.920</td>\n",
       "      <td>predicated 0.915</td>\n",
       "      <td>well-regulated 0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_50_wn_2_sg_1</th>\n",
       "      <td>educated 0.879</td>\n",
       "      <td>uneducated 0.863</td>\n",
       "      <td>well-ordered 0.860</td>\n",
       "      <td>well-endowed 0.831</td>\n",
       "      <td>well-informed 0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_50_wn_5_sg_0</th>\n",
       "      <td>educated 0.917</td>\n",
       "      <td>uneducated 0.900</td>\n",
       "      <td>well-planned 0.897</td>\n",
       "      <td>well-ordered 0.891</td>\n",
       "      <td>predicated 0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_50_wn_5_sg_1</th>\n",
       "      <td>educated 0.901</td>\n",
       "      <td>uneducated 0.876</td>\n",
       "      <td>caring 0.818</td>\n",
       "      <td>well-ordered 0.814</td>\n",
       "      <td>peasant 0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_50_wn_8_sg_0</th>\n",
       "      <td>educated 0.919</td>\n",
       "      <td>uneducated 0.893</td>\n",
       "      <td>well-planned 0.890</td>\n",
       "      <td>well-trained 0.873</td>\n",
       "      <td>well-ordered 0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_vs_50_wn_8_sg_1</th>\n",
       "      <td>educated 0.914</td>\n",
       "      <td>uneducated 0.898</td>\n",
       "      <td>caring 0.827</td>\n",
       "      <td>leisure 0.815</td>\n",
       "      <td>cared 0.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_100_wn_2_sg_0</th>\n",
       "      <td>middle-class 0.731</td>\n",
       "      <td>slum 0.722</td>\n",
       "      <td>kosovar 0.716</td>\n",
       "      <td>hiv-positive 0.716</td>\n",
       "      <td>distraught 0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_100_wn_2_sg_1</th>\n",
       "      <td>middle-class 0.864</td>\n",
       "      <td>cultured 0.853</td>\n",
       "      <td>remunerated 0.845</td>\n",
       "      <td>hiv-positive 0.836</td>\n",
       "      <td>spouses 0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_100_wn_5_sg_0</th>\n",
       "      <td>energy-efficient 0.630</td>\n",
       "      <td>middle-class 0.623</td>\n",
       "      <td>malawian 0.612</td>\n",
       "      <td>cultured 0.608</td>\n",
       "      <td>widows 0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_100_wn_5_sg_1</th>\n",
       "      <td>garment 0.804</td>\n",
       "      <td>cultured 0.801</td>\n",
       "      <td>remunerated 0.798</td>\n",
       "      <td>four-fifths 0.793</td>\n",
       "      <td>upbringing 0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_100_wn_8_sg_0</th>\n",
       "      <td>educated 0.602</td>\n",
       "      <td>abducting 0.549</td>\n",
       "      <td>over-populated 0.541</td>\n",
       "      <td>hospitable 0.534</td>\n",
       "      <td>filipino 0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_100_wn_8_sg_1</th>\n",
       "      <td>minimally 0.788</td>\n",
       "      <td>middle-class 0.788</td>\n",
       "      <td>owning 0.786</td>\n",
       "      <td>gainful 0.784</td>\n",
       "      <td>garment 0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_200_wn_2_sg_0</th>\n",
       "      <td>density 0.680</td>\n",
       "      <td>middle-class 0.656</td>\n",
       "      <td>multi-cultural 0.642</td>\n",
       "      <td>nomadic 0.626</td>\n",
       "      <td>homogenous 0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_200_wn_2_sg_1</th>\n",
       "      <td>middle-class 0.820</td>\n",
       "      <td>third-class 0.800</td>\n",
       "      <td>innovators 0.799</td>\n",
       "      <td>cultured 0.793</td>\n",
       "      <td>devout 0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_200_wn_5_sg_0</th>\n",
       "      <td>slum 0.509</td>\n",
       "      <td>400,000 0.493</td>\n",
       "      <td>school-age 0.487</td>\n",
       "      <td>450,000 0.485</td>\n",
       "      <td>distraught 0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_200_wn_5_sg_1</th>\n",
       "      <td>able-bodied 0.756</td>\n",
       "      <td>middle-class 0.740</td>\n",
       "      <td>upbringing 0.737</td>\n",
       "      <td>educated 0.737</td>\n",
       "      <td>underfed 0.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_200_wn_8_sg_0</th>\n",
       "      <td>educated 0.475</td>\n",
       "      <td>school-age 0.422</td>\n",
       "      <td>adults 0.412</td>\n",
       "      <td>peasant 0.408</td>\n",
       "      <td>recreation 0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_200_wn_8_sg_1</th>\n",
       "      <td>minimally 0.738</td>\n",
       "      <td>able-bodied 0.732</td>\n",
       "      <td>educated 0.731</td>\n",
       "      <td>owning 0.730</td>\n",
       "      <td>literate 0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_50_wn_2_sg_0</th>\n",
       "      <td>cultured 0.768</td>\n",
       "      <td>demographically 0.760</td>\n",
       "      <td>slum 0.760</td>\n",
       "      <td>resource-poor 0.748</td>\n",
       "      <td>crust 0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_50_wn_2_sg_1</th>\n",
       "      <td>middle-class 0.896</td>\n",
       "      <td>cultured 0.887</td>\n",
       "      <td>minimally 0.877</td>\n",
       "      <td>adolescent 0.870</td>\n",
       "      <td>nepali 0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_50_wn_5_sg_0</th>\n",
       "      <td>cultured 0.771</td>\n",
       "      <td>kosovar 0.755</td>\n",
       "      <td>kazakh 0.755</td>\n",
       "      <td>hire 0.754</td>\n",
       "      <td>toiling 0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_50_wn_5_sg_1</th>\n",
       "      <td>cultured 0.882</td>\n",
       "      <td>minimally 0.877</td>\n",
       "      <td>customer 0.877</td>\n",
       "      <td>garment 0.866</td>\n",
       "      <td>educated 0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_50_wn_8_sg_0</th>\n",
       "      <td>vilified 0.689</td>\n",
       "      <td>survivor 0.677</td>\n",
       "      <td>uneducated 0.677</td>\n",
       "      <td>dishonoured 0.672</td>\n",
       "      <td>weeping 0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_vs_50_wn_8_sg_1</th>\n",
       "      <td>garment 0.825</td>\n",
       "      <td>gainful 0.823</td>\n",
       "      <td>educated 0.820</td>\n",
       "      <td>underfed 0.820</td>\n",
       "      <td>malawian 0.811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0                      1  \\\n",
       "ft_vs_100_wn_2_sg_0       well-ordered 0.914         educated 0.903   \n",
       "ft_vs_100_wn_2_sg_1           educated 0.855       uneducated 0.841   \n",
       "ft_vs_100_wn_5_sg_0           educated 0.905       uneducated 0.878   \n",
       "ft_vs_100_wn_5_sg_1           educated 0.874       uneducated 0.854   \n",
       "ft_vs_100_wn_8_sg_0           educated 0.905       uneducated 0.875   \n",
       "ft_vs_100_wn_8_sg_1           educated 0.883       uneducated 0.854   \n",
       "ft_vs_200_wn_2_sg_0           educated 0.893     well-ordered 0.882   \n",
       "ft_vs_200_wn_2_sg_1           educated 0.856       uneducated 0.832   \n",
       "ft_vs_200_wn_5_sg_0           educated 0.895       uneducated 0.868   \n",
       "ft_vs_200_wn_5_sg_1           educated 0.854       uneducated 0.828   \n",
       "ft_vs_200_wn_8_sg_0           educated 0.894       uneducated 0.858   \n",
       "ft_vs_200_wn_8_sg_1           educated 0.871       uneducated 0.846   \n",
       "ft_vs_50_wn_2_sg_0        well-planned 0.922     well-ordered 0.921   \n",
       "ft_vs_50_wn_2_sg_1            educated 0.879       uneducated 0.863   \n",
       "ft_vs_50_wn_5_sg_0            educated 0.917       uneducated 0.900   \n",
       "ft_vs_50_wn_5_sg_1            educated 0.901       uneducated 0.876   \n",
       "ft_vs_50_wn_8_sg_0            educated 0.919       uneducated 0.893   \n",
       "ft_vs_50_wn_8_sg_1            educated 0.914       uneducated 0.898   \n",
       "w2v_vs_100_wn_2_sg_0      middle-class 0.731             slum 0.722   \n",
       "w2v_vs_100_wn_2_sg_1      middle-class 0.864         cultured 0.853   \n",
       "w2v_vs_100_wn_5_sg_0  energy-efficient 0.630     middle-class 0.623   \n",
       "w2v_vs_100_wn_5_sg_1           garment 0.804         cultured 0.801   \n",
       "w2v_vs_100_wn_8_sg_0          educated 0.602        abducting 0.549   \n",
       "w2v_vs_100_wn_8_sg_1         minimally 0.788     middle-class 0.788   \n",
       "w2v_vs_200_wn_2_sg_0           density 0.680     middle-class 0.656   \n",
       "w2v_vs_200_wn_2_sg_1      middle-class 0.820      third-class 0.800   \n",
       "w2v_vs_200_wn_5_sg_0              slum 0.509          400,000 0.493   \n",
       "w2v_vs_200_wn_5_sg_1       able-bodied 0.756     middle-class 0.740   \n",
       "w2v_vs_200_wn_8_sg_0          educated 0.475       school-age 0.422   \n",
       "w2v_vs_200_wn_8_sg_1         minimally 0.738      able-bodied 0.732   \n",
       "w2v_vs_50_wn_2_sg_0           cultured 0.768  demographically 0.760   \n",
       "w2v_vs_50_wn_2_sg_1       middle-class 0.896         cultured 0.887   \n",
       "w2v_vs_50_wn_5_sg_0           cultured 0.771          kosovar 0.755   \n",
       "w2v_vs_50_wn_5_sg_1           cultured 0.882        minimally 0.877   \n",
       "w2v_vs_50_wn_8_sg_0           vilified 0.689         survivor 0.677   \n",
       "w2v_vs_50_wn_8_sg_1            garment 0.825          gainful 0.823   \n",
       "\n",
       "                                         2                    3  \\\n",
       "ft_vs_100_wn_2_sg_0     well-planned 0.897   well-trained 0.896   \n",
       "ft_vs_100_wn_2_sg_1     well-ordered 0.810   well-trained 0.802   \n",
       "ft_vs_100_wn_5_sg_0     well-trained 0.872   well-planned 0.868   \n",
       "ft_vs_100_wn_5_sg_1     well-ordered 0.792   well-endowed 0.759   \n",
       "ft_vs_100_wn_8_sg_0     well-trained 0.860   well-planned 0.852   \n",
       "ft_vs_100_wn_8_sg_1       well-to-do 0.779   well-ordered 0.774   \n",
       "ft_vs_200_wn_2_sg_0       uneducated 0.879   well-advised 0.875   \n",
       "ft_vs_200_wn_2_sg_1     well-trained 0.756   well-ordered 0.754   \n",
       "ft_vs_200_wn_5_sg_0     well-planned 0.848   well-trained 0.847   \n",
       "ft_vs_200_wn_5_sg_1     well-trained 0.748   well-ordered 0.729   \n",
       "ft_vs_200_wn_8_sg_0     well-trained 0.842   well-planned 0.825   \n",
       "ft_vs_200_wn_8_sg_1          educate 0.746   well-ordered 0.739   \n",
       "ft_vs_50_wn_2_sg_0          educated 0.920     predicated 0.915   \n",
       "ft_vs_50_wn_2_sg_1      well-ordered 0.860   well-endowed 0.831   \n",
       "ft_vs_50_wn_5_sg_0      well-planned 0.897   well-ordered 0.891   \n",
       "ft_vs_50_wn_5_sg_1            caring 0.818   well-ordered 0.814   \n",
       "ft_vs_50_wn_8_sg_0      well-planned 0.890   well-trained 0.873   \n",
       "ft_vs_50_wn_8_sg_1            caring 0.827        leisure 0.815   \n",
       "w2v_vs_100_wn_2_sg_0         kosovar 0.716   hiv-positive 0.716   \n",
       "w2v_vs_100_wn_2_sg_1     remunerated 0.845   hiv-positive 0.836   \n",
       "w2v_vs_100_wn_5_sg_0        malawian 0.612       cultured 0.608   \n",
       "w2v_vs_100_wn_5_sg_1     remunerated 0.798    four-fifths 0.793   \n",
       "w2v_vs_100_wn_8_sg_0  over-populated 0.541     hospitable 0.534   \n",
       "w2v_vs_100_wn_8_sg_1          owning 0.786        gainful 0.784   \n",
       "w2v_vs_200_wn_2_sg_0  multi-cultural 0.642        nomadic 0.626   \n",
       "w2v_vs_200_wn_2_sg_1      innovators 0.799       cultured 0.793   \n",
       "w2v_vs_200_wn_5_sg_0      school-age 0.487        450,000 0.485   \n",
       "w2v_vs_200_wn_5_sg_1      upbringing 0.737       educated 0.737   \n",
       "w2v_vs_200_wn_8_sg_0          adults 0.412        peasant 0.408   \n",
       "w2v_vs_200_wn_8_sg_1        educated 0.731         owning 0.730   \n",
       "w2v_vs_50_wn_2_sg_0             slum 0.760  resource-poor 0.748   \n",
       "w2v_vs_50_wn_2_sg_1        minimally 0.877     adolescent 0.870   \n",
       "w2v_vs_50_wn_5_sg_0           kazakh 0.755           hire 0.754   \n",
       "w2v_vs_50_wn_5_sg_1         customer 0.877        garment 0.866   \n",
       "w2v_vs_50_wn_8_sg_0       uneducated 0.677    dishonoured 0.672   \n",
       "w2v_vs_50_wn_8_sg_1         educated 0.820       underfed 0.820   \n",
       "\n",
       "                                            4  \n",
       "ft_vs_100_wn_2_sg_0       well-informed 0.895  \n",
       "ft_vs_100_wn_2_sg_1          well-armed 0.776  \n",
       "ft_vs_100_wn_5_sg_0      well-regulated 0.858  \n",
       "ft_vs_100_wn_5_sg_1       well-informed 0.754  \n",
       "ft_vs_100_wn_8_sg_0        well-advised 0.835  \n",
       "ft_vs_100_wn_8_sg_1             educate 0.773  \n",
       "ft_vs_200_wn_2_sg_0        well-trained 0.874  \n",
       "ft_vs_200_wn_2_sg_1       well-informed 0.753  \n",
       "ft_vs_200_wn_5_sg_0      well-regulated 0.832  \n",
       "ft_vs_200_wn_5_sg_1             educate 0.726  \n",
       "ft_vs_200_wn_8_sg_0   well-orchestrated 0.802  \n",
       "ft_vs_200_wn_8_sg_1        well-trained 0.733  \n",
       "ft_vs_50_wn_2_sg_0       well-regulated 0.912  \n",
       "ft_vs_50_wn_2_sg_1        well-informed 0.827  \n",
       "ft_vs_50_wn_5_sg_0           predicated 0.886  \n",
       "ft_vs_50_wn_5_sg_1              peasant 0.799  \n",
       "ft_vs_50_wn_8_sg_0         well-ordered 0.854  \n",
       "ft_vs_50_wn_8_sg_1                cared 0.807  \n",
       "w2v_vs_100_wn_2_sg_0         distraught 0.705  \n",
       "w2v_vs_100_wn_2_sg_1            spouses 0.835  \n",
       "w2v_vs_100_wn_5_sg_0             widows 0.604  \n",
       "w2v_vs_100_wn_5_sg_1         upbringing 0.792  \n",
       "w2v_vs_100_wn_8_sg_0           filipino 0.534  \n",
       "w2v_vs_100_wn_8_sg_1            garment 0.780  \n",
       "w2v_vs_200_wn_2_sg_0         homogenous 0.622  \n",
       "w2v_vs_200_wn_2_sg_1             devout 0.789  \n",
       "w2v_vs_200_wn_5_sg_0         distraught 0.482  \n",
       "w2v_vs_200_wn_5_sg_1           underfed 0.732  \n",
       "w2v_vs_200_wn_8_sg_0         recreation 0.396  \n",
       "w2v_vs_200_wn_8_sg_1           literate 0.728  \n",
       "w2v_vs_50_wn_2_sg_0               crust 0.740  \n",
       "w2v_vs_50_wn_2_sg_1              nepali 0.867  \n",
       "w2v_vs_50_wn_5_sg_0             toiling 0.741  \n",
       "w2v_vs_50_wn_5_sg_1            educated 0.849  \n",
       "w2v_vs_50_wn_8_sg_0             weeping 0.662  \n",
       "w2v_vs_50_wn_8_sg_1            malawian 0.811  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if compare_topn.value == '':\n",
    "    #Take a random word in the vocab as a default\n",
    "    compare_topn.value = random.choice(list(list(models.values())[0].key_to_index))\n",
    "display(compare_topn)\n",
    "return_similar(compare_topn.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
